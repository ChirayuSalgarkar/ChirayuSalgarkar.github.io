\documentclass[10pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry}


\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}

\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{example}{Example}[section]


\title{Running Lecture Outline: Understanding RL}
\author{[Chirayu Salgarkar]}
\date{Fall 2024}

\begin{document}

\maketitle
\tableofcontents

\vspace{.25in}
\section{26-AUG-24}
\subsection{Order, Linear, and PDE vs ODE}
\section{28-AUG-24}

\subsection{Motvation}
Decision making is hard. How we tractably reason over a sequence of decisions is a subject for much research. One potential mechanism for modeling sequential decision making is a \textit{Markov Decision Process}. 
\subsection{MDP}
MDPS consist of a state $S$, action $A$, cost $C$, and transition$\mathcal{T}$.
\subsubsection{What is a state?}
A state refers to the sufficient statistic of the system to predict the future disregarding the past. This definition is not really precise. More generally, the state is the status of the world. That's a definition. Not the definition. 
We show state as $s \in S$.
\subsubsection{Action}
Action refers to the decisions or, more basically, the act of doing something, or the control action. 
\subsubsection{Cost}
The cost, or rewardd, is the instantaneous cost of an individual action within a state. This is denoted $c(s,a)$. Sometimes, we see $c(s,a,t)$ (time-dependence). Sometime, we even have $c(s,a,s',t)$ indicating previous state matters too. 
Dr. Baheri uses cost and reward interchangaby here. 
\subsubsection{Transition}
Insert Anthony Fantano joke here. The transition refers to the next state given the state and action. In a deterministic world, $s' = \mathcal{T}(s,a)$, but stochastic worlds are more of $s' \cong \mathcal{T}(s,a)$.

Quiz: Given a system, whate are the components of the MDP and how do you formulate them?

Let's identify the MDP components of Tetris.

State: Board configuration. 
Action: $4 * 10$
Cost: Userdefined. 
Transition: rule of game. Update of board game + random selection of next piece. 


For a self-driving car, what are the MDP components?

We now move to a Markov Decision Problem. This includes the things to define an optimization problem.
\subsection{MDP, continued}
We first describe the Horizon, and discount. 
\subsubsection{Horizon}
Simply when to make decision.
\subsubsection{Discount Factor}
Reward is more valuable at the current moment as opposed to the future! (Costs are more valuable when they happen soon.) They are represented as a $r \in \mathbb{R}$, $0 \leq r \leq 1$. Think of it this like 
\[c_0 + rc_1 + ... + r^{t-1}c_{t-1}\]

The final goal of RL is to find a $\textit{policy}$, essentially given state, what action do we have. We sek to find a policy that minimizes the sum of discounted future costs. 

\section{06-SEP-2024}
\subsection{Solving Markov Decision processes}
Solving an MDP, in general means to find a \textit{policy}. Recall the definition of policy. Then a MDP is a function map mapping decision to reward.  

But a better question is finding \textit{optimal policy}. What makes an optimal policy? The ultimate goal is to search over a family of policies in order to find the sequence of actions that maximizes (or sometimes minimize) the notion of the reward. That is, we seek to search over policies. Our goal in the slide is to minimize the Expected value. We then discuss discount factor. The sooner that we get the reward is generally a better choice. Discount value implies that future rewards mean that costs tend to matter less, as seen in previous slides. Therefore, in a case where we find optimal policies, we tend to work in the discounted version of the equation - that is we search over the poliies that maxes or mins the total sum of the cost.

So, how do we solve?
First, how NOT to solve: brute force. It simply takes far too long. We aren't doing bruteforce, because it takes far too long. 

We use the \textit{Bellman equation}.
\begin{defn}
    The Bellman equation is defined as:
    \[V^{\pi}(s_t) = \min_{a_t}(c(s_t, \pi(s_t)) + \gamma \mathbb{E}_{s_{t+1}} V^\pi(s_{t+1}))\]
    
\end{defn}
Dr. Baheri wants us to recognize how gorgeous this equation is. Next, we discuss value iteraion and policy iteration.


\end{document}
