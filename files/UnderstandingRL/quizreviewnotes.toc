\contentsline {section}{\numberline {1}Policy Gradient}{1}{}%
\contentsline {subsection}{\numberline {1.1}Policy Objective functions}{2}{}%
\contentsline {subsection}{\numberline {1.2}Finite Difference Policy Gradient}{2}{}%
\contentsline {subsection}{\numberline {1.3}Monte-Carlo Policy Gradient}{2}{}%
\contentsline {paragraph}{Derivation of the Score Function}{3}{}%
\contentsline {paragraph}{Why This is Useful}{3}{}%
\contentsline {paragraph}{Conclusion}{3}{}%
\contentsline {paragraph}{Softmax Policy with Linear Features}{3}{}%
\contentsline {paragraph}{Score Function}{3}{}%
\contentsline {paragraph}{Explanation}{4}{}%
\contentsline {paragraph}{Gaussian Policy}{4}{}%
\contentsline {paragraph}{Log-Probability of a Gaussian Policy}{4}{}%
\contentsline {paragraph}{Score Function for a Gaussian Policy}{4}{}%
\contentsline {paragraph}{Summary}{4}{}%
\contentsline {subsection}{\numberline {1.4}One-step MDPs}{5}{}%
\contentsline {paragraph}{One-Step MDPs}{5}{}%
\contentsline {paragraph}{Policy Optimization in One-Step MDPs}{5}{}%
\contentsline {paragraph}{One-Step MDPs}{5}{}%
\contentsline {paragraph}{Objective Function \( J(\theta ) \) in One-Step MDPs}{5}{}%
\contentsline {paragraph}{Multi-Step MDPs}{6}{}%
\contentsline {paragraph}{Policy Objective \( J(\theta ) \)}{6}{}%
\contentsline {paragraph}{Monte Carlo Policy Gradient (REINFORCE)}{6}{}%
\contentsline {paragraph}{Stochastic Gradient Ascent}{7}{}%
\contentsline {paragraph}{Summary}{7}{}%
\contentsline {subsection}{\numberline {1.5}Actor-Critic Policy Gradient}{7}{}%
\contentsline {paragraph}{Reducing Variance with a Critic}{7}{}%
\contentsline {paragraph}{Approximate Policy Gradient}{7}{}%
\contentsline {paragraph}{Summary}{7}{}%
\contentsline {section}{\numberline {2}Problems}{8}{}%
